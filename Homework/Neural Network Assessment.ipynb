{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we will dive into how neural networks work behind the scene. You will have a chance to implement functions vital to training a neural network: `forward`, `compute_loss`, `backward`, and `gradient_descent_step`; as well as implementing a small neural network for predicting the gravity of Mars!\n",
    "  \n",
    "The format of this homework will consist of function implementations and function unit-tests. There are no hidden tests - you receive full credit if you pass all the unit tests!\n",
    "<img src=\"https://miro.medium.com/max/791/0*hzIQ5Fs-g8iBpVWq.jpg\" alt=\"Neural Network\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents:  \n",
    "    - [Forward](#Forward)  \n",
    "    - [Compute Loss](#Compute-Loss)  \n",
    "    - [Backward](#Backward)  \n",
    "    - [Gradient Descent](#Gradient-Descent)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy matplotlib\n",
    "import numpy as np\n",
    "from ModelWrapper import check_answer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement forward propogation of a neural network with no biases - only weights $w$ and input $x$. Check with the instructor if you are lost!\n",
    "\n",
    "<img src=\"https://iartag.github.io/hek-ml-workshop/slides/static/images/3blue1brown_13_forward.gif\" alt=\"Forward\" style=\"width: 300px;\"/>\n",
    "  \n",
    "Fill out this `forward` function, and use the cell below it to check your answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w, x):\n",
    "    '''\n",
    "    @param w: weights of our neural net model\n",
    "    @param x: input data\n",
    "    \n",
    "    @return y: prediction calculated\n",
    "    '''\n",
    "    y = # TODO: calculate the prediction\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correctness of forward(w, x)\n",
    "\n",
    "x = np.array([0.10298026, 0.41655058, 0.48560227, 0.60588507, 0.8701086,\n",
    "           0.63899074, 0.32650349, 0.66185029, 0.43323724, 0.95059843])\n",
    "w = 0.83106743\n",
    "\n",
    "correct_answer = np.array([0.08558354, 0.34618162, 0.40356823, 0.50353135, 0.72311892,\n",
    "                           0.53104439, 0.27134642, 0.55004222, 0.36004936, 0.79001139])\n",
    "\n",
    "check_answer(correct_answer, forward(w, x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Mean Square Error loss given our `prediction`(from forward) and our `ground_truth`(the label).\n",
    "\n",
    "(Hint: MSE loss is defined as ...)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*WDKhO-z7rti70ZTv59yJ9A.jpeg\" alt=\"MSE Loss\" style=\"width: 400px;\"/>\n",
    "\n",
    "Fill out this `compute_loss` function, and use the cell below it to check your answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(prediction, ground_truth):\n",
    "    '''\n",
    "    @param prediction: prediction our model made\n",
    "    @param ground_truth: the real value corresponding to input data\n",
    "    \n",
    "    @return loss: the Mean Squared Error between prediction and ground_truth\n",
    "    '''\n",
    "    loss = # TODO: calculate the loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correctness of compute_loss(prediction, ground_truth)\n",
    "\n",
    "prediction = np.array([0.08558354, 0.34618162, 0.40356823, 0.50353135, 0.72311892,\n",
    "                       0.53104439, 0.27134642, 0.55004222, 0.36004936, 0.79001139])\n",
    "\n",
    "ground_truth = np.array([0.64265615, 0.38801768, 0.95016593, 0.824352  , 0.44688882,\n",
    "                         0.36782865, 0.90771466, 0.74559116, 0.52141405, 0.84289056])\n",
    "\n",
    "correct_answer = 0.12887562243845122\n",
    "check_answer(correct_answer, compute_loss(prediction, ground_truth))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement backward propogation of our neural network to compute the gradient of loss with respect to our weights $\\frac{d \\text{loss}}{d \\text{w}}$, given our weights used `w`, our inputs `x`, our label `ground_truth`, and the computed MSE `loss`. This question is very math heavy - use all your differentiation tools!\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*LB10KFg5J7yK1MLxTXcLdQ.jpeg\" alt=\"Backprop\" style=\"width: 400px;\"/>\n",
    "\n",
    "Fill out this `backward` function, and use the cell below it to check your answer!  \n",
    "  \n",
    "(Hint: if you are stuck, check out [the chain rule](https://www.mathtutor.ac.uk/differentiation/thechainrule), [the product rule](https://www.mathtutor.ac.uk/differentiation/theproductrule), and [the quotient rule](https://www.mathtutor.ac.uk/differentiation/thequotientrules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(w, x, ground_truth, loss):\n",
    "    '''\n",
    "    @param w: weights of our neural net model\n",
    "    @param x: input data\n",
    "    @param ground_truth: the real value corresponding to input data\n",
    "    @param loss: the Mean Squared Error between prediction and ground_truth\n",
    "    \n",
    "    @return gradient: (dl/dw) gradient of MSE loss against w\n",
    "    '''\n",
    "    gradient = # TODO: calculate the gradient\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correctness of backward(w, x, ground_truth, loss)\n",
    "\n",
    "x = np.array([0.10298026, 0.41655058, 0.48560227, 0.60588507, 0.8701086,\n",
    "           0.63899074, 0.32650349, 0.66185029, 0.43323724, 0.95059843])\n",
    "w = 0.83106743\n",
    "\n",
    "ground_truth = np.array([0.64265615, 0.38801768, 0.95016593, 0.824352  , 0.44688882,\n",
    "                         0.36782865, 0.90771466, 0.74559116, 0.52141405, 0.84289056])\n",
    "\n",
    "correct_answer = -0.12946738659087742\n",
    "check_answer(correct_answer, backward(w, x, ground_truth, compute_loss(prediction, ground_truth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent to optimize our neural network for minimum loss.\n",
    "\n",
    "<img src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_04-GradientDescent-WHITEBG_0.png\" alt=\"GradDesc\" style=\"width: 250px;\"/>\n",
    "\n",
    "Fill out this `gradient_descent_step` function, and use the cell below it to check your answer!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(w, gradient, learning_rate):\n",
    "    '''\n",
    "    @param w: weights of our neural net model\n",
    "    @param gradient: (dl/dw) gradient of MSE loss against w\n",
    "    @param learning_rate: learning rate for gradient descent algorithm\n",
    "    \n",
    "    @return updated_w: new weights for our model after taking one gradient descent step\n",
    "    '''\n",
    "    updated_w = # TODO: update the weights\n",
    "    return updated_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correctness of gradient_descent_step(w, graident, learning_rate)\n",
    "\n",
    "w = 0.83106743\n",
    "gradient = -0.12946738659087742\n",
    "learning_rate = 1e-4\n",
    "\n",
    "correct_answer = 0.8310803767386591\n",
    "check_answer(correct_answer, gradient_descent_step(w, gradient, learning_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

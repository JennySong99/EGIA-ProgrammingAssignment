{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we will dive into how neural networks work behind the scene. You will have a chance to implement functions vital to training a neural network: `forward`, `compute_loss`, `backward`, and `gradient_descent_step`; as well as implementing a small neural network for predicting the gravity of Mars!\n",
    "  \n",
    "The format of this homework will consist of function implementations and function unit-tests. There are no hidden tests - you receive full credit if you pass all the unit tests!\n",
    "<img src=\"https://miro.medium.com/max/791/0*hzIQ5Fs-g8iBpVWq.jpg\" alt=\"Neural Network\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents:  \n",
    "    - [Forward](#Forward)  \n",
    "    - [Compute Loss](#Compute-Loss)  \n",
    "    - [Backward](#Backward)  \n",
    "    - [Gradient Descent](#Gradient-Descent)  \n",
    "    - [Toy Example: Gravity of Mars](#Toy-Example:-Gravity-of-Mars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy matplotlib\n",
    "import numpy as np\n",
    "from ModelWrapper import check_answer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement forward propogation of a neural network with no biases - only weights $w$ and input $x$. Check with the instructor if you are lost!\n",
    "\n",
    "<img src=\"https://iartag.github.io/hek-ml-workshop/slides/static/images/3blue1brown_13_forward.gif\" alt=\"Forward\" style=\"width: 300px;\"/>\n",
    "  \n",
    "Fill out this `forward` function, and use the cell below it to check your answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w, x):\n",
    "    '''\n",
    "    @param w: weights of our neural net model\n",
    "    @param x: input data\n",
    "    \n",
    "    @return y: prediction calculated\n",
    "    '''\n",
    "    y = # TODO: calculate the prediction\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correctness of forward(w, x)\n",
    "\n",
    "x = np.array([0.10298026, 0.41655058, 0.48560227, 0.60588507, 0.8701086,\n",
    "           0.63899074, 0.32650349, 0.66185029, 0.43323724, 0.95059843])\n",
    "w = 0.83106743\n",
    "\n",
    "correct_answer = np.array([0.08558354, 0.34618162, 0.40356823, 0.50353135, 0.72311892,\n",
    "                           0.53104439, 0.27134642, 0.55004222, 0.36004936, 0.79001139])\n",
    "\n",
    "check_answer(correct_answer, forward(w, x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Mean Square Error loss given our `prediction`(from forward) and our `ground_truth`(the label).\n",
    "\n",
    "(Hint: MSE loss is defined as ...)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*WDKhO-z7rti70ZTv59yJ9A.jpeg\" alt=\"MSE Loss\" style=\"width: 400px;\"/>\n",
    "\n",
    "Fill out this `compute_loss` function, and use the cell below it to check your answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(prediction, ground_truth):\n",
    "    '''\n",
    "    @param prediction: prediction our model made\n",
    "    @param ground_truth: the real value corresponding to input data\n",
    "    \n",
    "    @return loss: the Mean Squared Error between prediction and ground_truth\n",
    "    '''\n",
    "    loss = # TODO: calculate the loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correctness of compute_loss(prediction, ground_truth)\n",
    "\n",
    "prediction = np.array([0.08558354, 0.34618162, 0.40356823, 0.50353135, 0.72311892,\n",
    "                       0.53104439, 0.27134642, 0.55004222, 0.36004936, 0.79001139])\n",
    "\n",
    "ground_truth = np.array([0.64265615, 0.38801768, 0.95016593, 0.824352  , 0.44688882,\n",
    "                         0.36782865, 0.90771466, 0.74559116, 0.52141405, 0.84289056])\n",
    "\n",
    "correct_answer = 0.12887562243845122\n",
    "check_answer(correct_answer, compute_loss(prediction, ground_truth))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement backward propogation of our neural network to compute the gradient of loss with respect to our weights $\\frac{d \\text{loss}}{d \\text{w}}$, given our weights used `w`, our inputs `x`, our label `ground_truth`, and the computed MSE `loss`. This question is very math heavy - use all your differentiation tools!\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*LB10KFg5J7yK1MLxTXcLdQ.jpeg\" alt=\"Backprop\" style=\"width: 400px;\"/>\n",
    "\n",
    "Fill out this `backward` function, and use the cell below it to check your answer!  \n",
    "  \n",
    "(Hint: if you are stuck, check out [the chain rule](https://www.mathtutor.ac.uk/differentiation/thechainrule), [the product rule](https://www.mathtutor.ac.uk/differentiation/theproductrule), and [the quotient rule](https://www.mathtutor.ac.uk/differentiation/thequotientrules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(w, x, ground_truth, loss):\n",
    "    '''\n",
    "    @param w: weights of our neural net model\n",
    "    @param x: input data\n",
    "    @param ground_truth: the real value corresponding to input data\n",
    "    @param loss: the Mean Squared Error between prediction and ground_truth\n",
    "    \n",
    "    @return gradient: (dl/dw) gradient of MSE loss against w\n",
    "    '''\n",
    "    gradient = # TODO: calculate the gradient\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correctness of backward(w, x, ground_truth, loss)\n",
    "\n",
    "x = np.array([0.10298026, 0.41655058, 0.48560227, 0.60588507, 0.8701086,\n",
    "           0.63899074, 0.32650349, 0.66185029, 0.43323724, 0.95059843])\n",
    "w = 0.83106743\n",
    "\n",
    "ground_truth = np.array([0.64265615, 0.38801768, 0.95016593, 0.824352  , 0.44688882,\n",
    "                         0.36782865, 0.90771466, 0.74559116, 0.52141405, 0.84289056])\n",
    "\n",
    "correct_answer = -0.12946738659087742\n",
    "check_answer(correct_answer, backward(w, x, ground_truth, compute_loss(prediction, ground_truth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent to optimize our neural network for minimum loss.\n",
    "\n",
    "<img src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_04-GradientDescent-WHITEBG_0.png\" alt=\"GradDesc\" style=\"width: 250px;\"/>\n",
    "\n",
    "Fill out this `gradient_descent_step` function, and use the cell below it to check your answer!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(w, gradient, learning_rate):\n",
    "    '''\n",
    "    @param w: weights of our neural net model\n",
    "    @param gradient: (dl/dw) gradient of MSE loss against w\n",
    "    @param learning_rate: learning rate for gradient descent algorithm\n",
    "    \n",
    "    @return updated_w: new weights for our model after taking one gradient descent step\n",
    "    '''\n",
    "    updated_w = # TODO: update the weights\n",
    "    return updated_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correctness of gradient_descent_step(w, graident, learning_rate)\n",
    "\n",
    "w = 0.83106743\n",
    "gradient = -0.12946738659087742\n",
    "learning_rate = 1e-4\n",
    "\n",
    "correct_answer = 0.8310803767386591\n",
    "check_answer(correct_answer, gradient_descent_step(w, gradient, learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Example: Gravity of Mars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA recently landed a rover on Mars! To emperically measure the gravity of Mars, NASA sent the rover with 100 objects of known mass(measured on Earth with gravity of Earth). The rover was able to measure the amount of gravitational forces experienced by each of these 100 objects. Help NASA find the gravity of Mars using gradient descent!\n",
    "\n",
    "<img src=\"https://s.yimg.com/os/creatr-uploaded-images/2020-07/131e69b0-cad0-11ea-a7f3-ff12d4122702\" alt=\"MarsRover\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data has already been collected for you here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collected\n",
    "gravitational_forces_measured = np.array([74.33922471, 18.89621021, 30.58944369, 87.37739668, 60.4725419 ,\n",
    "       57.6803083 , 56.2648722 , 93.00173316, 42.709132  , 41.79055808,\n",
    "       80.04357277, 71.69702041, 57.35612417, 45.43189827, 58.20043038,\n",
    "       64.31022133, 28.42197906, 42.53010491, 85.1058512 , 38.93001604,\n",
    "       49.52833613, 33.22718541, 91.75993885, 56.21887701, 80.93994588,\n",
    "       48.01562877, 18.9732248 , 57.74769716, 45.84018553, 91.60806876,\n",
    "       19.70652165, 37.60727816, 45.61403876, 69.8887408 , 52.77067505,\n",
    "       44.82916722, 18.85670526, 86.9777561 , 56.82597148, 65.92260198,\n",
    "       71.53819987, 64.0595462 , 22.58504336, 48.96056478, 28.25736909,\n",
    "       21.70609598, 38.96906147, 30.87682747, 76.77237865, 67.3654563 ])\n",
    "mass_known = np.array([19.97829966,  5.07833211,  8.22078435, 23.48221443, 16.25162994,\n",
    "       15.50120738, 15.1208723 , 24.9937833 , 11.47791295, 11.23109415,\n",
    "       21.51129403, 19.26826622, 15.4140919 , 12.20962345, 15.64103563,\n",
    "       17.28298684,  7.63826191, 11.42969585, 22.87174834, 10.46217718,\n",
    "       13.3105441 ,  8.9295792 , 24.66007791, 15.10851944, 21.75213904,\n",
    "       12.90390673,  5.09905799, 15.51950094, 12.31934303, 24.6191229 ,\n",
    "        5.29592859, 10.10680612, 12.25849186, 18.78223016, 14.18179067,\n",
    "       12.04753032,  5.0676313 , 23.37479313, 15.27177695, 17.71636206,\n",
    "       19.22556517, 17.21566366,  6.06971178, 13.15797054,  7.59393684,\n",
    "        5.83347426, 10.47273137,  8.29789311, 20.63209229, 18.10405342])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete the neural network training loop by fill out the lines marked with #TODO!\n",
    "\n",
    "#### (Hint: gravitational_forces_measured = gravity_of_mars * mass_known)\n",
    "\n",
    "You should be able to see the Gravity of Mars our neural network converged to, and our loss/weight trajectory as it converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 10 # initial w\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 30\n",
    "\n",
    "loss_trajectory = []\n",
    "w_trajectory = []\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    prediction = # TODO: Use our current model to predict\n",
    "    loss =       # TODO: Compute the loss between our prediction and the ground truth\n",
    "    gradient =   # TODO: Compute gradients for our model parameters\n",
    "    w =          # TODO: Update our model parameters with the computed gradients\n",
    "    loss_trajectory.append(loss)\n",
    "    w_trajectory.append(w)\n",
    "\n",
    "plt.title(\"Gradient Descent: Loss\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(loss_trajectory)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Gradient Descent: Weight\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.plot(w_trajectory)\n",
    "\n",
    "print(f\"Gravity of Mars found: {w}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations on mastering the keys to neural networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
